{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping News Articles from NPR and National Review\n",
    "#### December 9, 2018\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The data I will scrape will be used to do a text analysis between a politically right-leaning news website versus a left-leaning news website. \n",
    "\n",
    "Given the time constraints, I will be sampling one right-leaning site (National Review) and one left-leaning site (NPR). Let's begin by loading our libraries. Our main scraping tools will be Selenium and BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping NPR\n",
    "\n",
    "NPR is first and we will navigate straight to their article archive page. Let's set the browser driver and initiate a window to navigate to NPR's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set browser driver and navigate to NPR\n",
    "browser = webdriver.Chrome(executable_path=r\"/Users/chesterpoon/chromedriver\")\n",
    "browser.get('https://www.npr.org/sections/politics/archive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from manual navigation that to load the older articles, we have to scroll down to the bottom of the page where more articles will load. This can get very tedious when done manually, so we'll create a page down function to do this quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set page down function\n",
    "def pg_dwn():\n",
    "    browser.find_element_by_css_selector(\n",
    "        'body').send_keys(Keys.PAGE_DOWN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set the function in a for loop. I'll set the range limit to 50. I'll also add a time.sleep() function to give the page some time to render.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    for i in range(15):\n",
    "        pg_dwn()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for what we're about to do, I will first instantiate some empty lists to prepare for our pull. These will serve as our columns in our future data frame.\n",
    "\n",
    "Now that our list of articles have rendered, I'll write code to locate all the links for each article. A quick glance at the DOM and we can see that all articles are set in an \"h2\" tag with a class name of \"title\". We'll use beautifulsoup4 to get the page source and pull each link and store it in a list. We'll also pull the link text, which also happens to be our article titles and store that in a list as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_l = []\n",
    "datepub_l = []\n",
    "article_l = []\n",
    "href_l = [] \n",
    "\n",
    "source = browser.page_source\n",
    "html = bs4.BeautifulSoup(source, \"lxml\")\n",
    "links_l = html.find_all('h2', attrs={'class': 'title'})\n",
    "for l in links_l:\n",
    "    href_l.append(l.a.get('href'))\n",
    "for t in range(len(links_l)):\n",
    "    tl = links_l[t].get_text()\n",
    "    title_l.append(tl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From further manual navigation of the site, I noticed that when you navigate to an article and then back out back to the original archive page, our large list of articles are no longer rendered and we have to loop through our \"page down\" function again. To avoid this, we simply need to open each link we collected in a new browser tab.\n",
    "\n",
    "With each tab we open, we'll scrape the date the article was published, which is stored in a \"time\" tag. Because this is NPR and a number of the \"time\" tags indicate the time it takes for an audio playback to complete, the correct time tag is most likely our last one in all time tags on the page. We'll also store all the article contents into a variable, which is located between \"p\" tags without a class name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(href_l)):\n",
    "    browser.execute_script(\"window.open('\" + href_l[j] + \"', 'new_window')\")\n",
    "    browser.switch_to.window(browser.window_handles[1])\n",
    "\n",
    "    #start NPR loop here\n",
    "    source = browser.page_source\n",
    "    html = bs4.BeautifulSoup(source, \"lxml\")\n",
    "    datepub = html.find_all('time')[-1]['datetime']\n",
    "    content = html.find_all('p',attrs={'class': None})\n",
    "\n",
    "    article = \"\"\n",
    "\n",
    "    for c in content:\n",
    "        article = article + c.getText()\n",
    "\n",
    "\n",
    "    datepub_l.append(datepub)\n",
    "    article_l.append(article)\n",
    "    \n",
    "    browser.close()\n",
    "    browser.switch_to.window(browser.window_handles[0])\n",
    "    #     end loop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all our data is collected, we'll create a dataframe, do a bit of cleaning up and export it to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npr = pd.DataFrame({\n",
    "    'title': title_l,\n",
    "    'datepub': datepub_l,\n",
    "    'article': article_l,\n",
    "    'link': href_l\n",
    "})\n",
    "\n",
    "for index,row in npr.iterrows():\n",
    "    if row['datepub'][0] == 'P':\n",
    "        row['datepub'] = ''\n",
    "        \n",
    "npr['datepub'] =  pd.to_datetime(npr['datepub'])\n",
    "\n",
    "npr.to_csv('npr.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the National Review\n",
    "\n",
    "Much like the process for NPR's site, we'll follow much of the same steps. I will outline a few critical differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get('https://www.nationalreview.com/politics-policy/')\n",
    "\n",
    "ntitle_l = []\n",
    "ndatepub_l = []\n",
    "narticle_l = []\n",
    "nhref_l = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our first major difference is in how to load more articles. The National Review loads more articles when you scroll to the bottom of the screen and click the load more button. This time we'll loop through clicking the load more button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(60):\n",
    "    nxt = browser.find_element_by_css_selector('span.button-text')\n",
    "    nxt.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The National Review site has many ads that make their pull a bit difficult in that sometimes their page doesn't fully load because one of their ads timed out. It's also far slower than the NPR site, which I suspect is also due to the ads and clickbait. To handle these exceptions, I've built it exception handlers to tackle this issue. The rest of the code is identical to NPR's scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scrape each page\n",
    "source = browser.page_source\n",
    "html = bs4.BeautifulSoup(source, \"lxml\")\n",
    "links = html.find_all('h4', attrs={'class': 'post-list-article__title'})\n",
    "\n",
    "for t in range(len(links)):\n",
    "    tl = links[t].get_text()\n",
    "    ntitle_l.append(tl)\n",
    "\n",
    "for l in links:\n",
    "    nhref_l.append(l.a.get('href'))\n",
    "\n",
    "for j in range(len(nhref_l)):\n",
    "    browser.execute_script(\"window.open('\" + nhref_l[j] + \"', 'new_window')\")\n",
    "    browser.switch_to.window(browser.window_handles[1])\n",
    "\n",
    "    #start loop here\n",
    "    #handle time-out exceptions\n",
    "    try:\n",
    "        source = browser.page_source\n",
    "    except:\n",
    "        ndatepub_l.append('')\n",
    "        narticle_l.append('')\n",
    "        browser.close()\n",
    "        browser.switch_to.window(browser.window_handles[0])\n",
    "    else:\n",
    "        html = bs4.BeautifulSoup(source, \"lxml\")\n",
    "        #This handles a missing time tag exception\n",
    "        try:\n",
    "            datepub = html.find('time')['datetime'] \n",
    "        except:\n",
    "            datepub = ''\n",
    "        content = html.find_all('p',attrs={'class': None})\n",
    "\n",
    "        article = \"\"\n",
    "\n",
    "        for c in content:\n",
    "            article = article + c.getText()\n",
    "\n",
    "        ndatepub_l.append(datepub)\n",
    "        narticle_l.append(article)\n",
    "\n",
    "        browser.close()\n",
    "        browser.switch_to.window(browser.window_handles[0])\n",
    "    #     end loop here\n",
    "\n",
    "natrev = pd.DataFrame({\n",
    "    'title': ntitle_l,\n",
    "    'datepub': ndatepub_l,\n",
    "    'article': narticle_l,\n",
    "    'link': nhref_l\n",
    "})\n",
    "\n",
    "natrev['datepub'] =  pd.to_datetime(natrev['datepub'])\n",
    "natrev.to_csv('natrev.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
